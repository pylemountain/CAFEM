{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPS Feb 2022 Multi-Classifier Stacked Ensemble\n\n#### Random Forrest Classifer + KNeighbors Classifier + DecisionTree Classifier\n\n# Multi-Classifier Stacked Ensemble\n\nCredits: \n- [@lucamassaron](https://www.kaggle.com/lucamassaron/basic-eda-and-model-to-start)  Notebook.\n- [@pjofrelora](https://www.kaggle.com/pjofrelora/hybrid-classifier-solution-11th-place) Notebook.\n","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade scikit-learn\n!pip install mlxtend ","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-11T02:49:46.189027Z","iopub.execute_input":"2022-02-11T02:49:46.18945Z","iopub.status.idle":"2022-02-11T02:50:12.347721Z","shell.execute_reply.started":"2022-02-11T02:49:46.189353Z","shell.execute_reply":"2022-02-11T02:50:12.3468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.svm import SVC\nimport optuna\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n# from sklearn.model_selection import cross_val_score\n\nimport re\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-02-11T02:50:14.236201Z","iopub.execute_input":"2022-02-11T02:50:14.236828Z","iopub.status.idle":"2022-02-11T02:50:14.256544Z","shell.execute_reply.started":"2022-02-11T02:50:14.23678Z","shell.execute_reply":"2022-02-11T02:50:14.254837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-feb-2022/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-feb-2022/test.csv\")\nsubmission = pd.read_csv(\"../input/tabular-playground-series-feb-2022/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-11T02:50:14.258827Z","iopub.execute_input":"2022-02-11T02:50:14.259119Z","iopub.status.idle":"2022-02-11T02:50:56.679634Z","shell.execute_reply.started":"2022-02-11T02:50:14.259058Z","shell.execute_reply":"2022-02-11T02:50:56.678231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Features","metadata":{}},{"cell_type":"code","source":"# credit to Luca Massaron https://www.kaggle.com/lucamassaron/basic-eda-and-model-to-start\nfeatures = train.columns[1:-1]\n\ndef split_feature(st):\n    counts = list(map(int, re.split('A|T|G|C', st)[1:]))\n    return counts\n\nfeat2counts = {c: split_feature(c) for c in features}\n\na = [0 for i in range(11)]\nt = [0 for i in range(11)]\ng = [0 for i in range(11)]\nc = [0 for i in range(11)]\n\nfor feat in features:\n    xa, xt, xg, xc = feat2counts[feat]\n    a[xa] += 1\n    t[xt] += 1\n    g[xt] += 1\n    c[xc] += 1","metadata":{"execution":{"iopub.status.busy":"2022-02-11T02:50:56.696666Z","iopub.execute_input":"2022-02-11T02:50:56.697299Z","iopub.status.idle":"2022-02-11T02:50:56.859919Z","shell.execute_reply.started":"2022-02-11T02:50:56.697237Z","shell.execute_reply":"2022-02-11T02:50:56.858866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.29153Z","iopub.status.idle":"2022-02-11T00:54:07.292669Z","shell.execute_reply.started":"2022-02-11T00:54:07.292401Z","shell.execute_reply":"2022-02-11T00:54:07.29243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scaling(data):\n    scaler = StandardScaler()\n    scaler.fit_transform(data)\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.293893Z","iopub.status.idle":"2022-02-11T00:54:07.294692Z","shell.execute_reply.started":"2022-02-11T00:54:07.294426Z","shell.execute_reply":"2022-02-11T00:54:07.294455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splits","metadata":{}},{"cell_type":"code","source":"label_encoder = LabelEncoder()\nX = train[features]\ny = label_encoder.fit_transform(train['target'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T02:50:56.861465Z","iopub.execute_input":"2022-02-11T02:50:56.862028Z","iopub.status.idle":"2022-02-11T02:51:00.111029Z","shell.execute_reply.started":"2022-02-11T02:50:56.861979Z","shell.execute_reply":"2022-02-11T02:51:00.109863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scaler_f = StandardScaler()\n# X_train_scaled = scaler_f.fit_transform(X_train)\n# X_f_test_scaled = scaler_f.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.297441Z","iopub.status.idle":"2022-02-11T00:54:07.297922Z","shell.execute_reply.started":"2022-02-11T00:54:07.297713Z","shell.execute_reply":"2022-02-11T00:54:07.297741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SGDClassifier","metadata":{}},{"cell_type":"code","source":"# SGDCLASSIFIER WITH DEFAULT PARAMETERS\nmodel = SGDClassifier()\nmodel.fit(X_train,y_train)\npreds = model.predict(X_test)\nscore = accuracy_score(y_test,preds)\nprint(f'The  SGDClassifier Score with Default Settings is: {score}')\n\n# The  SGDClassifier Score with Default Settings is: 0.53248","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.29931Z","iopub.status.idle":"2022-02-11T00:54:07.299792Z","shell.execute_reply.started":"2022-02-11T00:54:07.299577Z","shell.execute_reply":"2022-02-11T00:54:07.299618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'penalty':'l1',\n          'alpha': 1.0021057779463507e-06,\n          'l1_ratio': 0.24745913938509045,\n          'fit_intercept':False,\n          'max_iter':1118,\n          'tol':0.001972030065242609,\n          'epsilon':0.017820792086488825,\n          'learning_rate':'optimal',\n          'eta0':0.009335633911125081,\n          'power_t':0.5706596138303384,\n    }\n\nmodel = SGDClassifier(**params)\nmodel.fit(X_train,y_train)\npreds = model.predict(X_test)\nscore = accuracy_score(y_test,preds)\nprint(f'The  SGDClassifier Score with Optimized Settings is: {score}')\n\n# The  SGDClassifier Score with Optimized Settings is: 0.81652","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.300747Z","iopub.status.idle":"2022-02-11T00:54:07.301222Z","shell.execute_reply.started":"2022-02-11T00:54:07.301009Z","shell.execute_reply":"2022-02-11T00:54:07.301036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    \n    params = {'loss':'log',\n                   # TRY USING:  'hinge','log','modified_huber','squared_hinge','perceptron',\n                   # 'squared_error','huber','epsilon_insensitive','squared_epsilon_insensitive']),  # default = hinge\n                   # The ‘log’ loss gives logistic regression, a probabilistic classifier.\n                   # ‘modified_huber’ is another smooth loss that brings tolerance to outliers as well as probability estimates.\n                   # ‘squared_hinge’ is like hinge but is quadratically penalized. ‘perceptron’ is the linear loss used by\n                   # the perceptron algorithm.   The other losses are designed for regression but can be useful in\n                   # classification as well; see SGDRegressor for a description.\n              'penalty':trial.suggest_categorical('penalty',['l2','l1','elasticnet']),                                          # default=’l2’\n                   # Defaults to ‘l2’ which is the standard regularizer for linear SVM models.\n                   # ‘l1’ and ‘elasticnet’ might bring sparsity to the model (feature selection) not achievable with ‘l2’.                         \n              'alpha':trial.suggest_float('alpha',low=.000001,high=.00001),                                                        # default = .0001\n                   # Constant that multiplies the regularization term. The higher the value, the stronger the regularization.\n                   # Also used to compute the learning rate when set to learning_rate is set to ‘optimal’.\n              'l1_ratio':trial.suggest_float('l1_ratio',low=.01,high=.3),                                                        # default =0.15\n                   # The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty,\n                   # l1_ratio=1 to L1. Only used if penalty is ‘elasticnet’.\n              'fit_intercept':trial.suggest_categorical('fit_intercept',[True,False]),                                                   # default=True\n              'max_iter':trial.suggest_int('max_iter', low=750,high=1250),                                                      # default =1000\n              'tol':trial.suggest_float('tol', low=1e-5,high=1e-2),                                                             # default ==1e-3\n              'shuffle':False,                                                                                                  # default = True\n              'random_state':21,                                                                      \n              'epsilon':trial.suggest_float('epsilon', .0001,.02),                                                                 # default=0.1 \n                   # insensitive loss functions; only if loss is ‘huber’, ‘epsilon_insensitive’, or\n                   # ‘squared_epsilon_insensitive’. For ‘huber’, determines the threshold at which it becomes less important \n                   # to get the prediction exactly right. For epsilon-insensitive, any differences between\n                   # the current prediction and the correct label are ignored if they are less than this threshold.                                 \n              'n_jobs':-1,                                                                                                        # default = None\n              'learning_rate':trial.suggest_categorical('learning_rate',['optimal','constant','invscaling','adaptive',]),         # default = optimal\n                   \n                   # The learning rate schedule:  ‘constant’: eta = eta0, ‘optimal’: eta = 1.0 / (alpha * (t + t0)) where t0 is \n                   #  chosen by a heuristic proposed by Leon Bottou.  ‘invscaling’: eta = eta0 / pow(t, power_t),  \n                   # ‘adaptive’: eta = eta0, as long as the training keeps decreasing. Each time n_iter_no_change consecutive \n                   # epochs fail to decrease the training loss by tol or fail to increase validation score by tol if \n                   # early_stopping is True, the current learning rate is divided by 5.   # The stopping criterion. \n                   # If it is not None, training will stop when (loss > best_loss - tol) for n_iter_no_change consecutive epochs.\n                   # Convergence is checked against the training loss or the validation loss depending on the early_stopping parameter.\n\n              'eta0':trial.suggest_float('eta0', 0.0,.01),  # default = 0.0\n                   # The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules. The default value is\n                   # 0.0 as eta0 is not used by the default schedule ‘optimal’.\n              'power_t':trial.suggest_float('power_t',low=.3,high=.6),                                                        # default = 0.5\n                   # The exponent for inverse scaling learning rate\n              'early_stopping':False,                                                                                           # default = False\n              'validation_fraction':.1,\n              'n_iter_no_change':5,\n              'class_weight':None,                                                                                               # default = None \n              'warm_start':False,                                                                                                # default = False\n              'average':False,                                                                                                   # default = False\n                   # When set to True, computes the averaged SGD weights across all updates and stores the result in \n                   # the coef_ attribute. If set to an int greater than 1, averaging will begin once the total number\n                   # of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples.\n             }\n    \n    model = SGDClassifier(**params)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    score = accuracy_score(y_test,preds)\n    return score\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=500)\n\n#Trial 290 finished with value: 0.81474 and parameters: {'penalty': 'l1', 'alpha': 1.0021057779463507e-06, \n# 'l1_ratio': 0.24745913938509045, 'fit_intercept': False, 'max_iter': 1118, 'tol': 0.001972030065242609, 'epsilon': 0.017820792086488825,\n# 'learning_rate': 'optimal', 'eta0': 0.009335633911125081, 'power_t': 0.5706596138303384}. Best is trial 290 with value: 0.81474.\n\n# Trial 94 finished with value: 0.81424 and parameters: {'penalty': 'l1', 'alpha':\n# 1.0006350245834825e-06, 'l1_ratio': 0.057477672736118965, 'fit_intercept': False, 'max_iter': 868, 'tol': 0.0031220764708228548, \n# 'epsilon': 0.007338482082787139, 'learning_rate': 'optimal', 'eta0': 0.0007703062792938397, 'power_t': 0.43945672471052566}.\n# Best is trial 94 with value: 0.81424.\n\n# Trial 123 finished with value: 0.81384 and parameters: {'penalty': 'l1', 'alpha': 1.0023399588644886e-06,\n# 'l1_ratio': 0.18197411568391297, 'fit_intercept': False, 'max_iter': 785, 'tol': 0.009833159597166106, 'epsilon': 0.005376745902229565,\n# 'learning_rate': 'optimal', 'eta0': 0.007795334297890297, 'power_t': 0.3451170716923369}. Best is trial 123 with value: 0.81384.\n\n# Trial 265 finished with value: 0.81748 and parameters: {'penalty': 'l1', 'alpha': \n# 1.0014951755556242e-06, 'l1_ratio': 0.14740252540229948, 'fit_intercept': False, 'max_iter': 783, 'tol': 0.0001231328444699676, \n# 'shuffle': True, 'epsilon': 0.015069998046005633, 'learning_rate': 'optimal', 'eta0': 0.008821276696654192, 'ccp_alpha': \n# 0.595934263631081}. Best is trial 265 with value: 0.81748.\n\n# Trial 105 finished with value: 0.8137 and parameters: {'penalty': 'l1', 'alpha': 1.0349576851162957e-05,\n# 'l1_ratio': 0.15237129057146087, 'fit_intercept': False, 'max_iter': 813, 'tol': 0.004471825647149898, 'shuffle': True, 'epsilon':\n# 0.02652709040960106, 'learning_rate': 'optimal', 'eta0': 0.003962948802914954, 'ccp_alpha': 0.4071901065880417}. Best is trial 105 with value: 0.8137.","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.30234Z","iopub.status.idle":"2022-02-11T00:54:07.303015Z","shell.execute_reply.started":"2022-02-11T00:54:07.30282Z","shell.execute_reply":"2022-02-11T00:54:07.302847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression Classifier","metadata":{}},{"cell_type":"code","source":"# Errors = PROBLEMS WITH THE SKLEARN LOGISTICREGRESSION MODULE\n\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\npreds = model.predict(X_test)\nscore = accuracy_score(y_test,preds)\nprint(f'The Logistic Regression Score with Default Settings and \"solver\" = \"\" is: {score}')\n\n# The Logistic Regression Score with Default Settings and \"solver\" = \"saga\" is: 0.64502\n# The Logistic Regression Score with Default Settings and \"solver\" = \"liblinear\" is: 0.63484\n# The Logistic Regression Score with Default Settings and \"solver\" = \"sag\" is: 0.64556\n# The Logistic Regression Score with Default Settings and \"solver\" = \"lbfgs\" is: error\n# The Logistic Regression Score with Default Settings and \"solver\" = \"newton-cg\" is: 0.64504","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.304264Z","iopub.status.idle":"2022-02-11T00:54:07.304599Z","shell.execute_reply.started":"2022-02-11T00:54:07.304424Z","shell.execute_reply":"2022-02-11T00:54:07.304448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'solver':'lbfgs',\n          'penalty':'l2',\n          ''\n    \n}\n\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\npreds = model.predict(X_test)\nscore = accuracy_score(y_test,preds)\nprint(f'The Logistic Regression Score is: {score}')","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.305782Z","iopub.status.idle":"2022-02-11T00:54:07.306182Z","shell.execute_reply.started":"2022-02-11T00:54:07.305982Z","shell.execute_reply":"2022-02-11T00:54:07.30602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optuna Optimization for Logistic Regression Classifier","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    \n    params = {'solver':'lbfgs',       # trial.suggest_categorical('solver','newton-cg','lbfgs','liblinear','sag','saga','sagd'), # default=’lbfgs’\n                    # For small datasets, ‘liblinear’ is a good choice, \n                    # whereas ‘sag’ and ‘saga’ are faster for large ones;\n                    # For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss;\n                    # ‘liblinear’ is limited to one-versus-rest schemes. \n                    # Warning The choice of the algorithm depends on the penalty chosen: Supported penalties by solver:\n                    # ‘newton-cg’ - [‘l2’, ‘none’] ,  ‘lbfgs’ - [‘l2’, ‘none’] , ‘liblinear’ - [‘l1’, ‘l2’], ‘sag’ - [‘l2’, ‘none’]\n                    # ‘saga’ - [‘elasticnet’, ‘l1’, ‘l2’, ‘none’]\n                    # Note ‘sag’ and ‘saga’ fast convergence is only guaranteed on features with approximately \n                    # the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.\n              'multi_class':'multinomial',                 # trial.suggest_categorical('multi_class',['auto','ovr','multinomial']), # default=’auto’\n                    # If the option chosen is ‘ovr’, then a binary problem is fit for each label.\n                    # For ‘multinomial’ the loss minimised is the multinomial loss fit across the entire \n                    # probability distribution, even when the data is binary. ‘multinomial’ is unavailable \n                    # when solver=’liblinear’. ‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’,\n                    # and otherwise selects ‘multinomial’. Stochastic Average Gradient descent solver for ‘multinomial’ case.\n              'penalty':'l2', # trial.suggest_categorical('penalty',['l2',None]),                                  # default = l2\n              'dual':False,                                            # trial.suggest_categorical('dual',True,False),  # default = False\n              'tol':1e-4,                                            # trial.suggest_int('tol',low=1e-5,high=1e-3),            # default = 1e-4\n              'C':trial.suggest_float('C',low=.95,high=1.05),                                                                    # default = 1.0\n              'fit_intercept':trial.suggest_categorical('fit_intercept',[True,False]),                                                  # default = True\n              # 'intercept_scaling':trial.suggest_float('intercept_scaling', low=.8,high=1.2),                                   # default = 1.0\n                     # Useful only when the solver ‘liblinear’ is used and\n                     # self.fit_intercept is set to True. \n              # 'class_weight':'auto',                                                                                          # default = None\n              'random_state':21, \n              'verbose': 0,\n              'warm_start':False,\n              'n_jobs':-1,\n              'max_iter':trial.suggest_int('max_iter',95,150),                                                                  # default = 100\n              # 'l1_ratio':trial.suggest_float('l1_ration',low=0, high=1),                                                        # default = None\n                   # The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. Only used \n                   # if penalty='elasticnet'. Setting l1_ratio=0 is equivalent to using penalty='l2',\n                   # while setting l1_ratio=1 is equivalent to using penalty='l1'. For 0 < l1_ratio <1,\n                   # the penalty is a combination of L1 and L2.\n             #  'class_weight':'balanced',                                                                                        # default = balanced\n              }\n\n    model = LogisticRegression(**params)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    score = accuracy_score(y_test,preds)\n    \n    return score\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=500)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T03:00:17.37749Z","iopub.execute_input":"2022-02-11T03:00:17.377803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"# RANDOM FOREST CLASSIFIER WITH DEFAULT SETTINGS\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\nscore = accuracy_score(y_test,preds)\nprint(f'Random Forrest Classifier Score with Default Settings is {score}')\n\n# Random Forrest Classifier Score with Default Settings is 0.99242","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.309005Z","iopub.status.idle":"2022-02-11T00:54:07.309383Z","shell.execute_reply.started":"2022-02-11T00:54:07.30916Z","shell.execute_reply":"2022-02-11T00:54:07.309182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tuned params for RandomForestClassifier\nparams = {'n_estimators':300,\n          'criterion':'gini',\n          'max_depth':None,\n          'min_samples_split': 2,\n          'min_samples_leaf':9,\n          'min_weight_fraction_leaf':0.0006226172051287282,\n          'max_leaf_nodes':574,\n          'min_impurity_decrease':0,\n          'bootstrap':'True',\n          'oob_score':'True',\n          'n_jobs':-1,\n          'verbose':0, \n          'warm_start':False,\n          'class_weight':'balanced', \n          'ccp_alpha':0,\n          'max_samples':None,\n         }\n\n\nmodel = RandomForestClassifier(**params)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\n\nscore = accuracy_score(y_test,preds)\nprint(f'Random Forrest Classifier Score is {score}')\n","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.310677Z","iopub.status.idle":"2022-02-11T00:54:07.311344Z","shell.execute_reply.started":"2022-02-11T00:54:07.31111Z","shell.execute_reply":"2022-02-11T00:54:07.311137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optuna Optimization Random Forrest Classifier","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    params = {'n_estimators':trial.suggest_int('n_estimators', low=90,high=110),       # default = 100\n             # 'max_features':16,\n              'criterion':'gini',                                                       # default = gini\n             #'criterion':trial.suggest_categorical('criterion',['gini','entropy']),\n              'max_depth':None,\n            #'max_depth':trial.suggest_int('max_depth',low=1,high=50),\n              'min_samples_split': 2,              #'min_samples_split':trial.suggest_float('min_samples_split',low=0,high=1.0),                                              \n              'min_samples_leaf':1,                # trial.suggest_int('min_samples_leaf', low=1,high=25),         \n              \n              'min_weight_fraction_leaf': 0.0,  # trial.suggest_float('min_weight_fraction_leaf', low=0,high=.01),\n            # 'max_features':'auto',\n              'max_leaf_nodes': None,                               # trial.suggest_int('max_leaf_nodes', low=200,high=600,step=1), default = None\n              'min_impurity_decrease':0.0,\n             #'min_impurity_decrease':trial.suggest_int('min_impurity_decrease',low=0, high=100),\n             #'bootstrap':'True',\n              'bootstrap':False,                                 # trial.suggest_categorical('bootstrap',[True,False]),\n             #'oob_score':'False',\n              'n_jobs':-1,\n              'random_state':21,\n              'verbose':0, \n              'warm_start':False,\n              'class_weight':'balanced_subsample',       # trial.suggest_categorical('balanced',['balanced','balanced_subsample', None]),\n              'ccp_alpha':0,\n            # 'ccp_alpha':trial.suggest_float('ccp_alpha',low=0.0,high=2.0),\n              'max_samples':None,\n             #'max_samples':trial.suggest_int('max_samples',low=1,high=500,step=10),\n             }\n    \n    model = RandomForestClassifier(**params)\n    model.fit(X_train, y_train)\n    print(model.score(X_test,y_test))\n    \n    return model.score(X_test, y_test)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=500)\n\n# [I 2022-02-11 00:50:20,713] Trial 15 finished with value: 0.99348 and parameters: {'n_estimators': 107}. Best is trial 15 with value: 0.99348.\n# [I 2022-02-11 00:21:36,903] Trial 11 finished with value: 0.99346 and parameters: {'n_estimators': 106, 'bootstrap': \n# False, 'balanced': 'balanced_subsample'}. Best is trial 11 with value: 0.99346.\n\n# [I 2022-02-11 00:06:10,228] Trial 10 finished with value: 0.99332 and parameters: {'n_estimators': 97, 'bootstrap': False}.\n# Best is trial 10 with value: 0.99332.","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.312634Z","iopub.status.idle":"2022-02-11T00:54:07.312949Z","shell.execute_reply.started":"2022-02-11T00:54:07.31278Z","shell.execute_reply":"2022-02-11T00:54:07.312803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MultinomialNB Classifier","metadata":{}},{"cell_type":"code","source":"# MULTINOMIAL NB WITH DEFAULT PARAMETERS\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\n\nscore = accuracy_score(y_test,preds)\nprint(f'KNeighbors Classifier Score is {score}')","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.314236Z","iopub.status.idle":"2022-02-11T00:54:07.3148Z","shell.execute_reply.started":"2022-02-11T00:54:07.314607Z","shell.execute_reply":"2022-02-11T00:54:07.314632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MULTINOMIAL NB WITH TUNED PARAMS\nparams = {\n    \n}\n\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\n\nscore = accuracy_score(y_test,preds)\nprint(f'KNeighbors Classifier Score is {score}')","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.315714Z","iopub.status.idle":"2022-02-11T00:54:07.31645Z","shell.execute_reply.started":"2022-02-11T00:54:07.316232Z","shell.execute_reply":"2022-02-11T00:54:07.316277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optuna Optimized MultiNomial NB","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    params = {'alpha':trial.suggest_float('alpha',.8,1.2),                                            # default=1.0\n          'fit_prior':trial.suggest_bool('fit_prior', True,False),                                # default= True\n          # 'class_prior':trial.suggest_int()                                                     # default = False\n         }\n\n    model = MultinomialNB()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n\n    score = accuracy_score(y_test,preds)\n    \nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=500)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.317312Z","iopub.status.idle":"2022-02-11T00:54:07.31804Z","shell.execute_reply.started":"2022-02-11T00:54:07.31786Z","shell.execute_reply":"2022-02-11T00:54:07.317881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KNeighbors Classifier","metadata":{}},{"cell_type":"code","source":"# no parameter input\nmodel = KNeighborsClassifier()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\nscore = accuracy_score(y_test,preds)\nprint(f'KNeighbors Classifier Score is {score}')\n\n# KNeighbors Classifier Score is 0.95542 with default settings","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.31888Z","iopub.status.idle":"2022-02-11T00:54:07.319181Z","shell.execute_reply.started":"2022-02-11T00:54:07.319022Z","shell.execute_reply":"2022-02-11T00:54:07.319038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# parameter input\nparams = {'n_neighbors':7,\n          'weights':'distance',\n          'algorithm':'brute', # try 'auto','ball_tree','kd_tree','brute'\n          'leaf_size':34,\n          'p':2,\n          'metric':'minkowski',\n          'n_jobs':-1,\n          }\n\nmodel = KNeighborsClassifier(**params)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\n\nscore = accuracy_score(y_test,preds)\nprint(f'KNeighbors Classifier Score is {score}')\n","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.320133Z","iopub.status.idle":"2022-02-11T00:54:07.32045Z","shell.execute_reply.started":"2022-02-11T00:54:07.320289Z","shell.execute_reply":"2022-02-11T00:54:07.320307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optuna Optimization for KNeighbors Classifier","metadata":{}},{"cell_type":"code","source":"# Optuna optimization function for KNeighborsClassifier\ndef objective(trial):\n    params = {'n_neighbors':trial.suggest_int('n_neighbors', low=3,high=8),                 # default = \n              'weights':trial.suggest_categorical('weights',['uniform','distance']),        # default = \n              'algorithm':trial.suggest_categorical('algorithm',['auto','ball_tree','kd_tree','brute']), # default = \n              'leaf_size':trial.suggest_int('leaf_size',20,40),                              # default = \n              'p':trial.suggest_int('p',low=1,high=3),                                       # default = \n              'metric':'minkowski',                                                          # default = \n              'n_jobs':-1,\n             }\n       \n    model = KNeighborsClassifier(**params)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    score = accuracy_score(y_test,preds)\n    \n    return score\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=500)\n\n\n# Trial 0 finished with value: 0.97026 and parameters: {'n_neighbors': 3, 'weights': 'uniform', 'algorithm': \n# 'auto', 'leaf_size': 25, 'p': 2}. Best is trial 0 with value: 0.97026.\n# [I 2022-02-09 13:46:47,630] Trial 1 finished with value: 0.98774 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'algorithm': 'brute',\n# 'leaf_size': 34, 'p': 2}. Best is trial 1 with value: 0.98774.","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.321415Z","iopub.status.idle":"2022-02-11T00:54:07.321702Z","shell.execute_reply.started":"2022-02-11T00:54:07.321548Z","shell.execute_reply":"2022-02-11T00:54:07.321564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"# DECISION TREE CLASSIFIER WITH DEFAULT SETTINGS\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\nscore = accuracy_score(y_test,preds)\nprint(f'Decision Tree Classifier Score with Default Settings is {score}')\n\n# Decision Tree Classifier Score with Default Settings is 0.9712","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.323327Z","iopub.status.idle":"2022-02-11T00:54:07.32397Z","shell.execute_reply.started":"2022-02-11T00:54:07.323755Z","shell.execute_reply":"2022-02-11T00:54:07.32378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'splitter':'best', # try 'random'\n          'criterion':'gini', # try 'entropy'\n          'max_depth':None, # try values 1 to 50\n         #'min_samples_split':0.0, # try values 0 to 1.0\n          'min_samples_leaf':1,  # try values 1 to 5\n         #'min_weight_fraction_leaf':None, # try values 0 to 5\n          'max_features':'auto',\n          'random_state':21,\n          'max_leaf_nodes':None,\n          'min_impurity_decrease':0, # try values 0 to 100\n          'class_weight':'balanced',\n          'ccp_alpha':0, # try values 0 to 2\n         }\n\nmodel = DecisionTreeClassifier(**params)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\nscore = accuracy_score(y_test,preds)\nprint(f'Decision Tree Classifier Score is {score}')\n\n# Decision Tree Classifier Score is 0.97048 - none\n# Decision Tree Classifier Score is 0.9691 'optimized'","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.324947Z","iopub.status.idle":"2022-02-11T00:54:07.325536Z","shell.execute_reply.started":"2022-02-11T00:54:07.325341Z","shell.execute_reply":"2022-02-11T00:54:07.325367Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    \n    params = {'splitter':trial.suggest_categorical('splitter',['best', 'random']),                  # default = \n              'criterion':'gini',                                                                   # default =\n              'max_depth':trial.suggest_int('max_depth',low=1,high=10),                             # default =\n              'min_samples_split':trial.suggest_float('min_samples_split',low=0,high=1.0),          # default =\n              'min_samples_leaf':trial.suggest_int('min_samples_leaf', low=1,high=5),               # default =\n              'min_weight_fraction_leaf':trial.suggest_float('min_weight_fraction_leaf', low=0,high=.5),# default =\n              'max_features':'auto',                                                                  # default =\n              'random_state':21,                                                                      # default =\n              'max_leaf_nodes':,                                                                      # default =\n              'min_impurity_decrease':trial.suggest_int('min_impurity_decrease',low=0, high=1),       # default =\n              'class_weight':'balanced',                                                              # default =\n              'ccp_alpha':trial.suggest_float('ccp_alpha',low=0.0,high=.001),                         # default =\n              }\n    \n    model = DecisionTreeClassifier(**params)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    score = accuracy_score(y_test,preds)\n    return score\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=500)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.326633Z","iopub.status.idle":"2022-02-11T00:54:07.327306Z","shell.execute_reply.started":"2022-02-11T00:54:07.32707Z","shell.execute_reply":"2022-02-11T00:54:07.327102Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVC Classifier","metadata":{}},{"cell_type":"code","source":"# svc_pipe = Pipeline(steps=[('standardscaler', StandardScaler()),('svc', SVC(gamma='auto'))])","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.328295Z","iopub.status.idle":"2022-02-11T00:54:07.328954Z","shell.execute_reply.started":"2022-02-11T00:54:07.328729Z","shell.execute_reply":"2022-02-11T00:54:07.328753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SVC CLASSIFIER WITH TUNED PARAMS\n# params = {\n    \n# }\n\n\n# svc_classifier = make_pipeline(svc_pipe)\n# svc_classifier.fit(X_train,y_train)\n# preds = svc_classifier.predict(X_test)\n# score = accuracy_score(y_test,preds)\n# print(f'The SVC Score with Parameter Tuning is: {score}')","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.329911Z","iopub.status.idle":"2022-02-11T00:54:07.330576Z","shell.execute_reply.started":"2022-02-11T00:54:07.330362Z","shell.execute_reply":"2022-02-11T00:54:07.330388Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# svc_classifier with default parameters\n# svc_classifier = make_pipeline(svc_pipe)\n# svc_classifier.fit(X_train,y_train)\n# preds = svc_classifier.predict(X_test)\n# score = accuracy_score(y_test,preds)\n# print(f'The SVC Score with Default Parameters is: {score}')\n\n# The SVC Score with Default Parameters is: 0.94672","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.331539Z","iopub.status.idle":"2022-02-11T00:54:07.33218Z","shell.execute_reply.started":"2022-02-11T00:54:07.331965Z","shell.execute_reply":"2022-02-11T00:54:07.33199Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optuna Optimized SVC Classifier","metadata":{}},{"cell_type":"code","source":"# svc_pipe = Pipeline(steps=[('standardscaler', StandardScaler()),('svc', SVC())])   \n# svc_classifier = make_pipeline(svc_pipe)\n\n# def objective(trial):\n    \n#     params = {'C':trial.suggest_float('C',.1,2.0),                                                           # default = 1.0\n#                    # Regularization parameter. The strength of the regularization is inversely \n#                    # proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n#               'kernel':trial.suggest_categorical('kernal',['linear','poly','rbf','sigmoid','precomputed']),      # default = rbf\n#                    # Specifies the kernel type to be used in the algorithm. If none is given,\n#                    # ‘rbf’ will be used. If a callable is given it is used to pre-compute \n#                    # the kernel matrix from data matrices; that matrix should be an array \n#                    # of shape (n_samples, n_samples).\n#               'degree':trial.suggest_int('degree',2,5),                                                    # default = 3\n#                    # Degree of the polynomial kernel function (‘poly’).\\\n#                    # Ignored by all other kernels. \n#               'gamma':trial.suggest_categorical('gamma',['scale','auto']),                                         # default = scale\n#                    # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.\n#                    # if gamma='scale' (default) is passed then it uses\n#                    # 1 / (n_features * X.var()) as value of gamma,\n#                    # if ‘auto’, uses 1 / n_features. \n#               'random_state':21,                                                                            # default = None\n#               'coef0':trial.suggest_float('coef0',0,1),                                                     # default = 0.0                          \n#                    # It is only significant in ‘poly’ and ‘sigmoid’.\n#               'shrinking':trial.suggest_categorical('shrinking',[True,False]),                                               # default = True\n#               'tol':trial.suggest_float('tol',1e-4,1e-2),                                            # default = 1e-3\n#               'class_weight':'balanced',                         # default = None\n#                    # Set the parameter C of class i to class_weight[i]*C for SVC. \n#                    # If not given, all classes are supposed to have weight one. \n#                    # The “balanced” mode uses the values of y to automatically adjust \n#                    # weights inversely proportional to class frequencies in the input \n#                    # data as n_samples / (n_classes * np.bincount(y)).\n              \n#               'max_iter':-1,                                                                         # default = -1\n#               'decision_function_shape':trial.suggest_categorical('decision_function_shape',['ovo','ovr']),                    # default = ovr\n#                    # decision_function_shape{‘ovo’, ‘ovr’}, default=’ovr’\n#                    # Whether to return a one-vs-rest (‘ovr’) decision function of\n#                    # shape (n_samples, n_classes) as all \n#                    # other classifiers, or the original one-vs-one (‘ovo’) decision \n#                    # function of libsvm which has shape\n#                    # (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\n#                    # (‘ovo’) is always used as multi-class strategy.\n#                    # The parameter is ignored for binary classification.\n             \n#              }\n\n    \n#     svc_classifier.fit(X_train,y_train)\n#     preds = svc_classifier.predict(X_test)\n#     score = accuracy_score(y_test,preds)\n    \n#     print(f'The SVC Score with Tuned Parameters is: {score}')\n    \n#     return score\n    \n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=500)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.333238Z","iopub.status.idle":"2022-02-11T00:54:07.333827Z","shell.execute_reply.started":"2022-02-11T00:54:07.333611Z","shell.execute_reply":"2022-02-11T00:54:07.333636Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ExtraTreesClassifier","metadata":{}},{"cell_type":"code","source":"# EXTRA TREES CLASSIFIER WITH DEFAULT SETTINGS\n# model = ExtraTreesClassifier()\n# model.fit(X_train, y_train)\n# preds = model.predict(X_test)\n# score = accuracy_score(y_test,preds)\n# print(f'Extra Trees Classifier Score is {score}')\n\n# Extra Trees Classifier Score is 0.99324 with default settings","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.334874Z","iopub.status.idle":"2022-02-11T00:54:07.335576Z","shell.execute_reply.started":"2022-02-11T00:54:07.335375Z","shell.execute_reply":"2022-02-11T00:54:07.335399Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EXTRA TREES CLASSIFIER WITH TUNED PARAMS\nparams = {\n    \n}","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.336545Z","iopub.status.idle":"2022-02-11T00:54:07.33705Z","shell.execute_reply.started":"2022-02-11T00:54:07.336869Z","shell.execute_reply":"2022-02-11T00:54:07.336888Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#OPTUNA TUNING FOR EXTRA TREES CLASSIFIER\ndef objective(trial):\n    \n    params = {'n_estimators':trial.suggest_int('n_estimators',low=95,high=105),   # default = 100\n              'criterion':'gini',              # trial.suggest_categorical('criterion',['gini','entropy']),  #  default = 'gini'\n              # 'max_depth':None,                # trial.suggest_float('max_depth',0.000001,2),\n              # 'min_samples_split':None,        # trial.suggest_float('min_samples_split',low=0.0,high=1),\n              # 'min_samples_leaf':None,         # trial.suggest_int('min_samples_leaf',1,3),\n              'min_weight_fraction_leaf':trial.suggest_float('min_weight_fraction_leaf', low=0,high=.001),\n              'max_features': 'auto',          # trial.suggest_categorical('max_features',['auto','sqrt','log2']),\n              'random_state':21,\n              'n_jobs':-1,\n              'bootstrap':trial.suggest_categorical('bootstrap',[True,False]),\n             }\n    \n    \n    model = ExtraTreesClassifier(**params)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    score = accuracy_score(y_test,preds)\n    \n    return score\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=500)\n\n# Trial 0 finished with value: 0.99292 and parameters: {'n_estimators': 95, 'bootstrap': False}. Best is trial 0 with value: 0.99292.\n# Trial 10 finished with value: 0.9929 and parameters: {'n_estimators': 105, 'min_weight_fraction_leaf':\n# 6.534368055766463e-06, 'bootstrap': True}. Best is trial 10 with value: 0.9929.","metadata":{"execution":{"iopub.status.busy":"2022-02-11T02:02:38.075376Z","iopub.execute_input":"2022-02-11T02:02:38.075903Z","iopub.status.idle":"2022-02-11T02:15:19.844739Z","shell.execute_reply.started":"2022-02-11T02:02:38.07584Z","shell.execute_reply":"2022-02-11T02:15:19.842496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# StackedClassifier Optimization and Parameters","metadata":{}},{"cell_type":"code","source":"\n\n\n\n# PARAMETERS AND OTHER SETTINGS FOR STACKEDCLASSIFIER **************** KEEP *******************\n# estimators = ['str',estimator]\n    # Base estimators which will be stacked together. Each element of the list is defined as \n    # a tuple of string (i.e. name) and an estimator instance. An estimator can be set to ‘drop’ using set_params.\n\n# final_estimatorestimator = final_estimator  -  default=None\n    # A classifier which will be used to combine the base estimators. The default classifier is a LogisticRegression.\n\n# cv = int, cross-validation generator or an iterable  -   default=None\n     # Determines the cross-validation splitting strategy used in cross_val_predict\n     # to train final_estimator. Possible inputs for cv are:\n            # None, to use the default 5-fold cross validation,\n            # integer, to specify the number of folds in a (Stratified) KFold,\n            # An object to be used as a cross-validation generator,\n            # An iterable yielding train, test splits.\n            # For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass,\n            # StratifiedKFold is used. In all other cases, KFold is used.\n            # These splitters are instantiated with shuffle=False so the splits will be the same across calls.\n\n\n# stack_method{‘auto’, ‘predict_proba’, ‘decision_function’, ‘predict’}, default=’auto’\n      # Methods called for each base estimator. It can be:\n            # if ‘auto’, it will try to invoke, for each estimator, 'predict_proba', 'decision_function' or 'predict' in that order.\n            # otherwise, one of 'predict_proba', 'decision_function' or 'predict'. If the method is not implemented by the estimator, it will raise an error.\n\n# n_jobs, default=None\n     # passthrough, default=False\n     # When False, only the predictions of estimators will be used as training data for final_estimator.\n     # When True, the final_estimator is trained on the predictions as well as the original training data.\n\n# verbose, default=0","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.339883Z","iopub.status.idle":"2022-02-11T00:54:07.340572Z","shell.execute_reply.started":"2022-02-11T00:54:07.340358Z","shell.execute_reply":"2022-02-11T00:54:07.340387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optuna Tuned Extra Trees Classifier","metadata":{}},{"cell_type":"markdown","source":"## Classifiers\n\n#### Random Forrest Classifier, KNeighbors Classifier, and Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"label_encoder = LabelEncoder()\nX = train[features]\ny = label_encoder.fit_transform(train['target'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify = y)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.341593Z","iopub.status.idle":"2022-02-11T00:54:07.342167Z","shell.execute_reply.started":"2022-02-11T00:54:07.341974Z","shell.execute_reply":"2022-02-11T00:54:07.342001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacked Ensemble","metadata":{}},{"cell_type":"code","source":"svc_pipe = Pipeline(steps=[('standard_scaler', StandardScaler()),('svc', SVC(gamma='auto'))])","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.343272Z","iopub.status.idle":"2022-02-11T00:54:07.343782Z","shell.execute_reply.started":"2022-02-11T00:54:07.343622Z","shell.execute_reply":"2022-02-11T00:54:07.343639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimators = [('RFC',RandomForestClassifier()),\n              ('KNC',KNeighborsClassifier()),\n              ('ETC',DecisionTreeClassifier()),\n              ('SVC', svc_pipe),]\n","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.344669Z","iopub.status.idle":"2022-02-11T00:54:07.345166Z","shell.execute_reply.started":"2022-02-11T00:54:07.344982Z","shell.execute_reply":"2022-02-11T00:54:07.345001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nstacked_classifier = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(solver='lbfgs'))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.346153Z","iopub.status.idle":"2022-02-11T00:54:07.34652Z","shell.execute_reply.started":"2022-02-11T00:54:07.346343Z","shell.execute_reply":"2022-02-11T00:54:07.34636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nstacked_classifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.347854Z","iopub.status.idle":"2022-02-11T00:54:07.348636Z","shell.execute_reply.started":"2022-02-11T00:54:07.34843Z","shell.execute_reply":"2022-02-11T00:54:07.348453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"preds = stacked_classifier.predict(test[features])\n\nsubmission.target = label_encoder.inverse_transform(preds)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T00:54:07.350038Z","iopub.status.idle":"2022-02-11T00:54:07.351048Z","shell.execute_reply.started":"2022-02-11T00:54:07.350768Z","shell.execute_reply":"2022-02-11T00:54:07.350799Z"},"trusted":true},"execution_count":null,"outputs":[]}]}